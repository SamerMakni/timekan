<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>timekan.models.tkan_lstm &#8212; timeKAN</title>
    <link rel="stylesheet" href="../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=649a27d8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/insipid.css?v=75f6bc3c" />

    <script src="../../../_static/documentation_options.js?v=38b66d78"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script defer src="../../../_static/insipid.js"></script>
    <script defer src="../../../_static/insipid-sidebar.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
  </head><body>
    <script type="text/javascript">
        document.body.classList.add('js');
    </script>
    <input type="checkbox" id="sidebar-checkbox" style="display: none;">
    <label id="overlay" for="sidebar-checkbox"></label>
    <div class="sidebar-resize-handle"></div>
    <script type="text/javascript">
        try {
            let sidebar = localStorage.getItem('sphinx-sidebar');
            const sidebar_width = localStorage.getItem('sphinx-sidebar-width');
            if (sidebar_width) {
                document.documentElement.style.setProperty('--sidebar-width', sidebar_width);
            }
            // show sidebar on wide screen
            if (!sidebar && window.matchMedia('(min-width: 100rem)').matches) {
                sidebar = 'visible';
                // NB: We don't store the value in localStorage!
            }
            if (sidebar === 'visible') {
                document.getElementById('sidebar-checkbox').checked = true;
            }
        } catch(e) {
            console.info(e);
        }
    </script>
    <header id="topbar-placeholder">
      <div id="topbar">
        <div id="titlebar">
          <div class="buttons">
            <label for="sidebar-checkbox" id="sidebar-button" role="button" tabindex="0" aria-controls="sphinxsidebar" accesskey="M">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/></svg>
            </label>
            <button id="search-button" type="button" title="Search" aria-label="Search" aria-expanded="false" aria-controls="search-form" accesskey="S">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
            </button>
          </div>
          <div class="title">
            <a class="parent" href="../../index.html" accesskey="U">Module code</a>
            <a class="top" href="#">timekan.models.tkan_lstm</a>
          </div>
          <div class="buttons">
            <button id="fullscreen-button" type="button" aria-hidden="true">
              <span class="enable">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M212.686 315.314L120 408l32.922 31.029c15.12 15.12 4.412 40.971-16.97 40.971h-112C10.697 480 0 469.255 0 456V344c0-21.382 25.803-32.09 40.922-16.971L72 360l92.686-92.686c6.248-6.248 16.379-6.248 22.627 0l25.373 25.373c6.249 6.248 6.249 16.378 0 22.627zm22.628-118.628L328 104l-32.922-31.029C279.958 57.851 290.666 32 312.048 32h112C437.303 32 448 42.745 448 56v112c0 21.382-25.803 32.09-40.922 16.971L376 152l-92.686 92.686c-6.248 6.248-16.379 6.248-22.627 0l-25.373-25.373c-6.249-6.248-6.249-16.378 0-22.627z"/></svg>
              </span>
              <span class="disable">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M4.686 427.314L104 328l-32.922-31.029C55.958 281.851 66.666 256 88.048 256h112C213.303 256 224 266.745 224 280v112c0 21.382-25.803 32.09-40.922 16.971L152 376l-99.314 99.314c-6.248 6.248-16.379 6.248-22.627 0L4.686 449.941c-6.248-6.248-6.248-16.379 0-22.627zM443.314 84.686L344 184l32.922 31.029c15.12 15.12 4.412 40.971-16.97 40.971h-112C234.697 256 224 245.255 224 232V120c0-21.382 25.803-32.09 40.922-16.971L296 136l99.314-99.314c6.248-6.248 16.379-6.248 22.627 0l25.373 25.373c6.248 6.248 6.248 16.379 0 22.627z"/></svg>
              </span>
            </button>
          </div>
        </div>
        <div id="searchbox" role="search">
          <form id="search-form" class="search" style="display: none" action="../../../search.html" method="get">
            <input type="search" name="q" placeholder="Search ..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" />
            <button>Go</button>
          </form>
        </div>
      </div>
    </header>
    <nav>
    </nav>

    <nav class="relbar">
    </nav>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
<h1>Source code for timekan.models.tkan_lstm</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">from</span> <span class="nn">..layers</span> <span class="kn">import</span> <span class="n">KANLinear</span><span class="p">,</span> <span class="n">ChebyKANLayer</span><span class="p">,</span> <span class="n">NaiveFourierKANLayer</span>



<span class="k">class</span> <span class="nc">TKANCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A LSTM cell enhanced with Kolmogorov-Arnold Network (KAN) layers.</span>

<span class="sd">    This class implements a single time step computation for an LSTM cell where</span>
<span class="sd">    KAN layers enhance the output gate computation.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_dim (int): Size of the input dimension.</span>
<span class="sd">        hidden_dim (int): Size of the hidden state dimension.</span>
<span class="sd">        kan_type (str, optional): Type of KAN layer (&#39;spline&#39;, &#39;chebychev&#39;, &#39;fourier&#39;). Defaults to &#39;fourier&#39;.</span>
<span class="sd">        sub_kan_configs (dict, optional): Configuration for KAN sub-layers. Defaults to None.</span>
<span class="sd">        sub_kan_output_dim (int, optional): Output dimension of KAN sub-layers. Defaults to None.</span>
<span class="sd">        sub_kan_input_dim (int, optional): Input dimension of KAN sub-layers. Defaults to None.</span>
<span class="sd">        activation (callable, optional): Activation for cell state. Defaults to torch.tanh.</span>
<span class="sd">        recurrent_activation (callable, optional): Activation for gates. Defaults to torch.sigmoid.</span>
<span class="sd">        use_bias (bool, optional): Whether to use bias in gates. Defaults to True.</span>
<span class="sd">        dropout (float, optional): Dropout rate for input. Defaults to 0.0.</span>
<span class="sd">        recurrent_dropout (float, optional): Dropout rate for recurrent connections. Defaults to 0.0.</span>
<span class="sd">        layer_norm (bool, optional): Whether to apply layer normalization. Defaults to False.</span>
<span class="sd">        num_sub_layers (int, optional): Number of KAN sub-layers. Defaults to 1.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; cell = TKANCell(input_dim=1, hidden_dim=16, kan_type=&#39;fourier&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(32, 1)  # batch_size=32, input_dim=1</span>
<span class="sd">        &gt;&gt;&gt; h, states = cell(x)</span>
<span class="sd">        &gt;&gt;&gt; print(h.shape)  # Expected: torch.Size([32, 16])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="n">kan_type</span><span class="p">,</span>
        <span class="n">sub_kan_configs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">sub_kan_output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">sub_kan_input_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
        <span class="n">recurrent_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">layer_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">num_sub_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_configs</span> <span class="o">=</span> <span class="n">sub_kan_configs</span> <span class="ow">or</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_input_dim</span> <span class="o">=</span> <span class="n">sub_kan_input_dim</span> <span class="ow">or</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_output_dim</span> <span class="o">=</span> <span class="n">sub_kan_output_dim</span> <span class="ow">or</span> <span class="n">input_dim</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recurrent_activation</span> <span class="o">=</span> <span class="n">recurrent_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">layer_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kan_type</span> <span class="o">=</span> <span class="n">kan_type</span>
    
        <span class="c1"># LSTM</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recurrent_kernel</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_configs</span> <span class="o">=</span> <span class="n">sub_kan_configs</span> <span class="ow">or</span> <span class="p">{}</span>
        
        <span class="k">if</span> <span class="n">use_bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">kan_type</span> <span class="o">==</span> <span class="s1">&#39;spline&#39;</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">KANLinear</span>
        <span class="k">elif</span> <span class="n">kan_type</span> <span class="o">==</span> <span class="s1">&#39;chebychev&#39;</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">ChebyKANLayer</span>
        <span class="k">elif</span> <span class="n">kan_type</span> <span class="o">==</span> <span class="s1">&#39;fourier&#39;</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">NaiveFourierKANLayer</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">tkan_sub_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_sub_layers</span><span class="p">):</span>
            <span class="n">layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputdim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_input_dim</span><span class="p">,</span> <span class="n">outdim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_output_dim</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_configs</span><span class="p">)</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">tkan_sub_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">sub_tkan_kernel</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tkan_sub_layers</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_output_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub_tkan_recurrent_kernel_inputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tkan_sub_layers</span><span class="p">),</span> <span class="n">input_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_input_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub_tkan_recurrent_kernel_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tkan_sub_layers</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_output_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_input_dim</span><span class="p">))</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">aggregated_weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tkan_sub_layers</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_output_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aggregated_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">))</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recurrent_dropout</span> <span class="o">=</span> <span class="n">recurrent_dropout</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ln</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the parameters of the TKANCell.</span>

<span class="sd">        Uses Kaiming uniform initialization for weights and zeros for biases.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">orthogonal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">recurrent_kernel</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub_tkan_kernel</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub_tkan_recurrent_kernel_inputs</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub_tkan_recurrent_kernel_states</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">aggregated_weight</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">aggregated_bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes one time step of the KAN-enhanced LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).</span>
<span class="sd">            states (list, optional): List of previous hidden state, cell state, and sub-states. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple: (hidden state, updated states), where hidden state is of shape (batch_size, hidden_dim)</span>
<span class="sd">                   and states is a list of tensors for the next step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">sub_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_kan_output_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> 
                          <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tkan_sub_layers</span><span class="p">))]</span>
            <span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">+</span> <span class="n">sub_states</span>
        
        <span class="n">h_tm1</span><span class="p">,</span> <span class="n">c_tm1</span><span class="p">,</span> <span class="o">*</span><span class="n">sub_states</span> <span class="o">=</span> <span class="n">states</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">recurrent_dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">h_tm1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">h_tm1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">recurrent_dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        
        <span class="n">gates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_tm1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">recurrent_kernel</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">gates</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        
        <span class="n">gates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">recurrent_activation</span><span class="p">(</span><span class="n">gates</span><span class="p">)</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">c_candidate</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">gates</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">c</span> <span class="o">=</span> <span class="n">f</span> <span class="o">*</span> <span class="n">c_tm1</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">c_candidate</span><span class="p">)</span>
        
        <span class="n">sub_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">new_sub_states</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">sub_layer</span><span class="p">,</span> <span class="n">sub_state</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tkan_sub_layers</span><span class="p">,</span> <span class="n">sub_states</span><span class="p">)):</span>
            <span class="n">sub_kernel_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_tkan_recurrent_kernel_inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="n">sub_kernel_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_tkan_recurrent_kernel_states</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            
            <span class="n">agg_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sub_kernel_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">sub_state</span><span class="p">,</span> <span class="n">sub_kernel_h</span><span class="p">)</span>
            <span class="n">sub_output</span> <span class="o">=</span> <span class="n">sub_layer</span><span class="p">(</span><span class="n">agg_input</span><span class="p">)</span>
            
            <span class="n">sub_recurrent_kernel_h</span><span class="p">,</span> <span class="n">sub_recurrent_kernel_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub_tkan_kernel</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">new_sub_state</span> <span class="o">=</span> <span class="n">sub_recurrent_kernel_h</span> <span class="o">*</span> <span class="n">sub_output</span> <span class="o">+</span> <span class="n">sub_state</span> <span class="o">*</span> <span class="n">sub_recurrent_kernel_x</span>
            
            <span class="n">sub_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sub_output</span><span class="p">)</span>
            <span class="n">new_sub_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_sub_state</span><span class="p">)</span>
        
        <span class="n">aggregated_sub_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">sub_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">aggregated_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">aggregated_sub_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregated_weight</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregated_bias</span>
        
        <span class="n">o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">recurrent_activation</span><span class="p">(</span><span class="n">aggregated_input</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">o</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">:</span>
            <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">h</span><span class="p">,</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">+</span> <span class="n">new_sub_states</span>


<div class="viewcode-block" id="tKANLSTM">
<a class="viewcode-back" href="../../../package_reference.html#timekan.models.tKANLSTM">[docs]</a>
<span class="k">class</span> <span class="nc">tKANLSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A KAN-enhanced LSTM model for time series processing.</span>

<span class="sd">    This class wraps TKANCell to process full sequences, with options for bidirectional processing</span>
<span class="sd">    and sequence output.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_dim (int): Size of the input dimension.</span>
<span class="sd">        hidden_dim (int): Size of the hidden state dimension.</span>
<span class="sd">        sub_kan_configs (dict, optional): Configuration for KAN sub-layers. Defaults to None.</span>
<span class="sd">        sub_kan_output_dim (int, optional): Output dimension of KAN sub-layers. Defaults to None.</span>
<span class="sd">        sub_kan_input_dim (int, optional): Input dimension of KAN sub-layers. Defaults to None.</span>
<span class="sd">        activation (callable, optional): Activation for cell state. Defaults to torch.tanh.</span>
<span class="sd">        recurrent_activation (callable, optional): Activation for gates. Defaults to torch.sigmoid.</span>
<span class="sd">        dropout (float, optional): Dropout rate for input. Defaults to 0.0.</span>
<span class="sd">        recurrent_dropout (float, optional): Dropout rate for recurrent connections. Defaults to 0.0.</span>
<span class="sd">        return_sequences (bool, optional): Whether to return the full sequence. Defaults to False.</span>
<span class="sd">        bidirectional (bool, optional): Whether to process bidirectionally. Defaults to False.</span>
<span class="sd">        layer_norm (bool, optional): Whether to apply layer normalization. Defaults to False.</span>
<span class="sd">        kan_type (str, optional): Type of KAN layer (&#39;spline&#39;, &#39;chebychev&#39;, &#39;fourier&#39;). Defaults to &#39;fourier&#39;.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; model = tKANLSTM(input_dim=1, hidden_dim=16, return_sequences=True, bidirectional=True)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(32, 10, 1)  # batch_size=32, seq_len=10, input_dim=1</span>
<span class="sd">        &gt;&gt;&gt; output = model(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)  # Expected: torch.Size([32, 10, 32]) due to bidirectional</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="n">sub_kan_configs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">sub_kan_output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">sub_kan_input_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
        <span class="n">recurrent_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">layer_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">kan_type</span><span class="o">=</span><span class="s1">&#39;fourier&#39;</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">TKANCell</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
            <span class="n">sub_kan_configs</span><span class="o">=</span><span class="n">sub_kan_configs</span><span class="p">,</span>
            <span class="n">sub_kan_output_dim</span><span class="o">=</span><span class="n">sub_kan_output_dim</span><span class="p">,</span>
            <span class="n">sub_kan_input_dim</span><span class="o">=</span><span class="n">sub_kan_input_dim</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">recurrent_activation</span><span class="o">=</span><span class="n">recurrent_activation</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">recurrent_dropout</span><span class="o">=</span><span class="n">recurrent_dropout</span><span class="p">,</span>
            <span class="n">layer_norm</span><span class="o">=</span><span class="n">layer_norm</span><span class="p">,</span>
            <span class="n">kan_type</span><span class="o">=</span><span class="n">kan_type</span>
        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">return_sequences</span> <span class="o">=</span> <span class="n">return_sequences</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
        
        <span class="k">if</span> <span class="n">bidirectional</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reverse_cell</span> <span class="o">=</span> <span class="n">TKANCell</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
                <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                <span class="n">sub_kan_configs</span><span class="o">=</span><span class="n">sub_kan_configs</span><span class="p">,</span>
                <span class="n">sub_kan_output_dim</span><span class="o">=</span><span class="n">sub_kan_output_dim</span><span class="p">,</span>
                <span class="n">sub_kan_input_dim</span><span class="o">=</span><span class="n">sub_kan_input_dim</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                <span class="n">recurrent_activation</span><span class="o">=</span><span class="n">recurrent_activation</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
                <span class="n">recurrent_dropout</span><span class="o">=</span><span class="n">recurrent_dropout</span><span class="p">,</span>
                <span class="n">layer_norm</span><span class="o">=</span><span class="n">layer_norm</span> <span class="p">,</span>
                <span class="n">kan_type</span><span class="o">=</span><span class="n">kan_type</span>
            <span class="p">)</span>
    
<div class="viewcode-block" id="tKANLSTM.forward">
<a class="viewcode-back" href="../../../package_reference.html#timekan.models.tKANLSTM.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">initial_states</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Processes an input sequence through the KAN-enhanced LSTM.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_dim).</span>
<span class="sd">            initial_states (list, optional): Initial states for forward pass. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Output tensor, shape depends on return_sequences and bidirectional settings.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">forward_states</span> <span class="o">=</span> <span class="n">initial_states</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">initial_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">forward_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="n">h_forward</span><span class="p">,</span> <span class="n">forward_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:],</span> <span class="n">forward_states</span><span class="p">)</span>
            <span class="n">forward_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h_forward</span><span class="p">)</span>
        
        <span class="n">forward_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">forward_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">forward_outputs</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_sequences</span> <span class="k">else</span> <span class="n">forward_outputs</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
        
        <span class="n">backward_states</span> <span class="o">=</span> <span class="n">initial_states</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">initial_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">backward_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">h_backward</span><span class="p">,</span> <span class="n">backward_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse_cell</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:],</span> <span class="n">backward_states</span><span class="p">)</span>
            <span class="n">backward_outputs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">h_backward</span><span class="p">)</span>
        
        <span class="n">backward_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">backward_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">combined_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">forward_outputs</span><span class="p">,</span> <span class="n">backward_outputs</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">combined_outputs</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_sequences</span> <span class="k">else</span> <span class="n">combined_outputs</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span></div>
</div>

</pre></div>
            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">

          <div class="sidebar-resize-handle"></div>

<h3><a href="../../../index.html">Table of Contents</a></h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package_reference.html">Package Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Examples</a></li>
</ul>

<div><hr class="docutils"></div>
<ul>
  <li class="toctree-l1"><a class="reference internal" href="../../../genindex.html" accesskey="I">General Index</a></li>
  <li class="toctree-l1"><a class="reference internal" href="../../../py-modindex.html">Python Module Index</a></li>
</ul>
<div id="ethical-ad-placement"></div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <nav class="relbar">
    </nav>

    <footer role="contentinfo">
      &#169; Copyright 2025, Samer Makni.
      Created using <a class="reference external" href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
    </footer>
  </body>
</html>